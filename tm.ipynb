{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic Modeling of oncologist bios using the BetterDoctor API**\n",
    "\n",
    "Latent Dirichlet allocation is an **automatic topic discovery algorithm** first described in a paper published in thhe Journal of Machine Learning Research by Dr. Blei in 2003. \n",
    "For example, in a newspaper, the topics might be Entertainment, Sports, Politics, Classifieds...\n",
    "Each topic would then have a list of top-20 words.\n",
    "\n",
    "**Suppose we wish to discover what topics describe the world of oncology.**\n",
    "\n",
    "Using the **BetterDoctor API**, we obtain a set of 100 doctors in the surrounding 100 mile radius of a given lat/long location:\n",
    "\n",
    "a. To obtain a fairly distributed set of oncologists, we look up the locations of the top-10 cities in the USA. \n",
    "b. We thus obtain 10 times 100 = 1000 doctor profiles. These are extremely rich profiles with insurances, practices, addresses, phone numbers, degrees etc. We narrow in on the doctor's bio, which describes each doctor succintly in a para. \n",
    "c. We split each bio into multiple sentences. Now we have a giant corpus of sentences that describe what oncologists do.\n",
    "\n",
    "We use LDA to extract topics from this corpus.\n",
    "But how does LDA perform this discovery?\n",
    "\n",
    "**Mechanism**\n",
    "LDA is a bag-of-words model.\n",
    "LDA represents documents as topic mixtures, that spit out words with certain probabilities. \n",
    "It assumes that documents are produced like so -\n",
    "Suppose you write a bio, you\n",
    "a. Decide on the number of words N the bio has, according to a Poisson distribution.\n",
    "b. Choose a topic mixture for the bio according to a Dirichlet distribution over a fixed set of K topics. \n",
    "Say we pick 12 words in a bio. \n",
    "\n",
    "We decide \n",
    "1/3 of the bio is about the doctor's credentials, \n",
    "1/3 about the locations where he practiced, and \n",
    "1/3 about what kind of diseases he specialises in.\n",
    "\n",
    "So picking a topic has 33% probability in the multinomial distribution of topics.\n",
    "Within a topic, we have a 25% probability of picking 1 of 4 words.\n",
    "Thus, each topic is probabilistically generated.\n",
    "\n",
    "Assuming this generative model for a collection of documents, LDA then tries to backtrack from the documents to find a set of topics that are likely to have generated the collection.\n",
    "\n",
    "**Learning**\n",
    "\n",
    "Now we have a set of bios. We choose a fixed number say 10 topics to automatically discover. LDA uses collapsed Gibbs sampling to discover the topic representation of each of these ten topics.\n",
    "\n",
    "LDA goes through each bio & randomly assigns each word in the bio to one of the 10 topics.\n",
    "\n",
    "This random assignment already gives you both topic representations of all the bios and word distributions of all the topics (albeit not very good ones).\n",
    "T improve on them, for each bio b,\n",
    "go through each word w in b…\n",
    "And for each topic t, compute two things: \n",
    "a) p(topic t | bio b) = the proportion of words in b that are currently assigned to topic t, \n",
    "b) p(word w | topic t) = the proportion of assignments to topic t over all bios that come from this word w.\n",
    "\n",
    "Reassign w a new topic, where we choose topic t with probability p(topic t | bio b) * p(word w | topic t)\n",
    "\n",
    "According to our generative model, this is essentially the probability that topic t generated word w, so it makes sense that we resample the current word’s topic with this probability. We’re assuming that all topic assignments except for the current word in question are correct, and then updating the assignment of the current word using our model of how documents are generated. After repeating the previous step a large number of times, we eventually reach a roughly steady state where all assignments are pretty good. \n",
    "\n",
    "So we use these assignments to estimate the topic mixtures of each document (by counting the proportion of words assigned to each topic within that document) and the words associated to each topic (by counting the proportion of words assigned to each topic overall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "medical active history malpractice passed screening licenses background check including license successfully addition having automated status elements looked hold verification\n",
      "Topic #1:\n",
      "jersey new combinatorial wyoming chemical chemistry course asge endoscopy voorhees like library cherry barcelona schleicher dua batezini hill spain orange\n",
      "Topic #2:\n",
      "medicine practice licensed california university medical minnesota school colorado degree new illinois washington palliative hospice graduated york received emergency neurology\n",
      "Topic #3:\n",
      "naqvi bilal practiced fortunate welcome locations settings web page burbank francesco federico variety privilege feel wong likes traveling golfing patchogue\n",
      "Topic #4:\n",
      "medical cancer oncology university center medicine clinical american hematology research internal completed board society texas fellowship patients residency hospital college\n",
      "Topic #5:\n",
      "nebraska specialize affiliated doctors hematologist rose iowa hospital hartford ridge practice years white received sky nambudiri filho costa braunfels degree\n",
      "Topic #6:\n",
      "patients licensed treat doctors united states based 20 experience network credentials rated university california medical pennsylvania washington minnesota colorado maryland\n",
      "Topic #7:\n",
      "oncology hematology medicine md internal currently patients california minnesota medical specializes specialist practices treats sees washington colorado surgical specializing illinois\n",
      "Topic #8:\n",
      "licenses active medical holds clear automated background check malpractice history addition having robinson goldstein hyde yen pollard rabinowe parikh cheng\n",
      "Topic #9:\n",
      "time family enjoys wife playing surgery son hiking enjoy husband world reading spending sons hobbies children skiing include spare love\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import urllib.request\n",
    "import json\n",
    "\n",
    "api_key = '8d0c5c7411a969ae233ee34d47b1784c'\n",
    "base_url = \"https://api.betterdoctor.com/2016-03-01/doctors?specialty_uid=oncologist&location=\"\n",
    "rest_url = \",100&limit=100&user_key=\" + api_key\n",
    "\n",
    "# curated list of lat/long of top-10 cities in USA\n",
    "nyc = (40.71,-74.00)\n",
    "sf = (37.77, -122.42)\n",
    "dc = (38.91, -77.03)\n",
    "boston = (42.36, -71.06)\n",
    "seattle = (47.61, -122.33)\n",
    "chicago = (41.88, -87.63)\n",
    "austin = (30.27, -97.74)\n",
    "la = (34.05,-118.24)\n",
    "denver = (39.74,-104.99)\n",
    "minn = (44.98, -93.26)\n",
    "\n",
    "top10cities = [nyc,sf,dc,boston,seattle,chicago,austin,la,denver,minn]\n",
    "bios = []\n",
    "\n",
    "for city in top10cities:\n",
    "    time.sleep(5)\n",
    "    link = base_url + str(city[0]) + \",\" + str(city[1]) + rest_url    \n",
    "    jsonstr = urllib.request.urlopen(link).read().decode(\"utf8\")\n",
    "    data = json.loads(jsonstr)['data']\n",
    "\n",
    "    for index in range(len(data)):        \n",
    "        bio = data[index]['profile']['bio']\n",
    "        for s in bio.split(\".\\n\\n\"):\n",
    "            bios.append(s)\n",
    "            \n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "\n",
    "# Load the doctor bios, vectorize it. \n",
    "data_samples = bios\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(stop_words='english', max_df=0.95)\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "lda = LatentDirichletAllocation(n_topics=10, max_iter=10,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, 20) #top 20 words per topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "TOPIC 1 - SUBJECTS\n",
    "jersey new combinatorial wyoming chemical chemistry course asge endoscopy voorhees like library cherry barcelona schleicher dua batezini hill spain orange\n",
    "\n",
    "TOPIC 2 - SCHOOLS, PLACES, LAST NAMES\n",
    "medicine practice licensed california university medical minnesota school colorado degree new illinois washington palliative hospice graduated york received emergency neurology\n",
    "medical cancer oncology university center medicine clinical american hematology research internal completed board society texas fellowship patients residency hospital college\n",
    "nebraska specialize affiliated doctors hematologist rose iowa hospital hartford ridge practice years white received sky nambudiri filho costa braunfels degree\n",
    "\n",
    "TOPIC 3 - LICENSES\n",
    "medical active history malpractice passed screening licenses background check including license successfully addition having automated status elements looked hold verification\n",
    "licenses active medical holds clear automated background check malpractice history addition having robinson goldstein hyde yen pollard rabinowe parikh cheng\n",
    "\n",
    "TOPIC 4 - LIFE\n",
    "time family enjoys wife playing surgery son hiking enjoy husband world reading spending sons hobbies children skiing include spare love\n",
    "\n",
    "TOPIC 5 - MEDICINE\n",
    "oncology hematology medicine md internal currently patients california minnesota medical specializes specialist practices treats sees washington colorado surgical specializing illinois"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
